<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Relaxify — Emotion Detection (Happy vs Sad)</title>
  <style>
    :root{--accent:#6C5CE7;--muted:#666;--bg:#f7f8fb;--card:#fff}
    body{font-family:Inter, system-ui, -apple-system, 'Segoe UI', Roboto, Arial; margin:0; background:var(--bg); color:#111}
    header{background:linear-gradient(90deg,var(--accent),#00b894); color:white; padding:2rem 1.5rem}
    .wrap{max-width:1000px;margin:0 auto;padding:2rem}
    nav a{color:rgba(255,255,255,0.95);margin-right:1rem;text-decoration:none;font-weight:600}
    h1{margin:0 0 .5rem}
    .lead{opacity:0.95}
    section{background:var(--card);box-shadow:0 6px 18px rgba(16,24,40,0.06);border-radius:12px;padding:1.5rem;margin:1.25rem 0}
    .grid{display:grid;grid-template-columns:1fr 1fr;gap:1rem;align-items:start}
    .center{text-align:center}
    .badge{display:inline-block;padding:.25rem .6rem;border-radius:999px;background:#eef2ff;color:var(--accent);font-weight:700}
    table{width:100%;border-collapse:collapse;margin-top:.75rem}
    th,td{padding:.6rem;border-bottom:1px solid #eee;text-align:left}
    .svg-face{width:140px;height:140px}
    footer{padding:2rem;text-align:center;color:var(--muted);font-size:.9rem}
    code{background:#f1f5f9;padding:.15rem .4rem;border-radius:6px;font-family:monospace}
    .btn{display:inline-block;padding:.5rem .9rem;border-radius:8px;background:var(--accent);color:white;text-decoration:none;font-weight:700}
    .mono{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, 'Roboto Mono', monospace}
    @media(max-width:780px){.grid{grid-template-columns:1fr}.svg-face{width:110px;height:110px}}
  </style>
</head>
<body>
  <header>
    <div class="wrap">
      <nav>
        <a href="#problem">Problem</a>
        <a href="#approach">Approach</a>
        <a href="#visualizations">Visualizations</a>
        <a href="#results">Results</a>
        <a href="#takeaways">Takeaways</a>
      </nav>
      <h1>Relaxify</h1>
      <p class="lead">Real-time emotion detection (Happy vs Sad) for personalized stress relief recommendations — powered by AlexNet.</p>
    </div>
  </header>

  <main class="wrap">

    <section id="problem">
      <h2>Problem & Motivation</h2>
      <div class="grid">
        <div>
          <p>Modern wellness apps offer static relaxation routines that do not adapt to a user's current emotional state. <strong>Relaxify</strong> aims to close this gap by automatically detecting a user's mood from a face image (happy vs. sad) and tailoring on-demand relaxation content (breathing exercises, music, micro-meditations).</p>
          <ul>
            <li>High prevalence of daily stress and negative mood states.</li>
            <li>Personalization increases engagement and efficacy.</li>
            <li>Low-latency, on-device inference enables real-time feedback.</li>
          </ul>
          <p><strong>Dataset (example):</strong> FER2013 (facial emotion dataset). For the Relaxify baseline we map labels to two classes: <span class="badge">Happy</span> and <span class="badge">Sad</span>.</p>
        </div>
        <div class="center">
          <!-- Happy face SVG -->
          <div style="display:flex;gap:1rem;justify-content:center;align-items:center;flex-wrap:wrap">
            <svg id="happy" class="svg-face" viewBox="0 0 120 120" xmlns="http://www.w3.org/2000/svg" role="img">
              <circle cx="60" cy="60" r="55" fill="#fff9e6" stroke="#ffd166" stroke-width="4"/>
              <circle cx="45" cy="50" r="6" fill="#222"/>
              <circle cx="75" cy="50" r="6" fill="#222"/>
              <path d="M40 78 Q60 95 80 78" stroke="#334155" stroke-width="4" fill="none" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>

            <svg id="sad" class="svg-face" viewBox="0 0 120 120" xmlns="http://www.w3.org/2000/svg" role="img">
              <circle cx="60" cy="60" r="55" fill="#f0f7ff" stroke="#74b9ff" stroke-width="4"/>
              <circle cx="45" cy="50" r="6" fill="#222"/>
              <circle cx="75" cy="50" r="6" fill="#222"/>
              <path d="M40 86 Q60 68 80 86" stroke="#334155" stroke-width="4" fill="none" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
          </div>
          <p style="margin-top:.5rem;color:var(--muted)">Sample happy / sad icons used in Relaxify UI</p>
        </div>
      </div>
    </section>

    <section id="approach">
      <h2>Technical Approach (AlexNet)</h2>
      <p><strong>Model:</strong> AlexNet — adapted to a binary classification (happy vs sad). We keep the original convolutional backbone and replace the final fully connected layer with a <code>2-way</code> softmax.</p>

      <div class="grid">
        <div>
          <h3>Architecture (summary)</h3>
          <table>
            <thead><tr><th>Stage</th><th>Details</th></tr></thead>
            <tbody>
              <tr><td>Input</td><td>RGB image, resized to <code>224×224</code></td></tr>
              <tr><td>Conv layers</td><td>5 conv blocks, ReLU activations, max pooling after certain layers</td></tr>
              <tr><td>FC layers</td><td>Two hidden FC layers (4096 units) + final FC with 2 outputs</td></tr>
              <tr><td>Regularization</td><td>Dropout (0.5), data augmentation (flips, rotations)</td></tr>
              <tr><td>Loss</td><td>Cross-Entropy Loss</td></tr>
              <tr><td>Optimizer</td><td>Adam (default) or SGD with momentum</td></tr>
            </tbody>
          </table>
        </div>
        <div>
          <h3>Training pipeline</h3>
          <ol>
            <li>Load dataset, remap labels to <em>happy</em>/<em>sad</em>.</li>
            <li>Preprocess: resize to <code>224×224</code>, normalize with ImageNet mean/std.</li>
            <li>Augment: random horizontal flip, small rotation, color jitter (optional).</li>
            <li>Train/validate split (e.g., 80/20). Track accuracy & loss per epoch.</li>
            <li>Experiment: test different batch sizes (16/64), learning rates (1e-3 / 1e-4), augmentation on/off, and compare to ResNet18 transfer learning baseline.</li>
          </ol>
          <p><strong>Outputs saved:</strong> model weights (.pth/.pt), training logs and plots, confusion matrix, and sample predictions.</p>
        </div>
      </div>

      <p style="margin-top:1rem">Below is a compact flow diagram of the pipeline.</p>
      <div style="display:block;padding:1rem;background:#fbfdff;border-radius:10px;margin-top:.5rem">
        <div class="mono">Raw images → Preprocess (resize, normalize) → Augmentation → AlexNet → Train (CrossEntropy) → Evaluate → Save weights + plots</div>
      </div>
    </section>

    <section id="visualizations">
      <h2>Visualizations</h2>
      <p>Example plots (placeholders below) — replace these with figures exported from your Colab notebook (PNG).</p>
      <div class="grid">
        <div>
          <h4>Training / Validation Accuracy</h4>
          <!-- Placeholder simple SVG plot -->
          <svg viewBox="0 0 300 140" width="100%" height="160" aria-hidden="true">
            <rect width="100%" height="100%" fill="#fff" rx="6"/>
            <polyline fill="none" stroke="#6C5CE7" stroke-width="3" points="10,120 40,95 80,70 120,55 160,45 200,38 240,33 280,30"/>
            <polyline fill="none" stroke="#00b894" stroke-width="3" points="10,125 40,115 80,100 120,90 160,85 200,82 240,80 280,78"/>
            <text x="12" y="18" font-size="12" fill="#222">Accuracy</text>
            <text x="18" y="135" font-size="10" fill="#999">Epoch →</text>
            <text x="40" y="30" font-size="10" fill="#6C5CE7">Train</text>
            <text x="40" y="40" font-size="10" fill="#00b894">Val</text>
          </svg>
        </div>
        <div>
          <h4>Confusion Matrix (placeholder)</h4>
          <!-- 2x2 confusion matrix -->
          <svg viewBox="0 0 220 160" width="100%" height="160" aria-hidden="true">
            <rect x="0" y="0" width="220" height="160" rx="6" fill="#fff"/>
            <rect x="30" y="30" width="70" height="70" fill="#dfe6ff"/>
            <rect x="120" y="30" width="70" height="70" fill="#ffe6e6"/>
            <rect x="30" y="110" width="70" height="30" fill="#fff"/>
            <rect x="120" y="110" width="70" height="30" fill="#fff"/>
            <text x="45" y="70" font-size="20">120</text>
            <text x="140" y="70" font-size="20">30</text>
            <text x="45" y="140" font-size="20">20</text>
            <text x="140" y="140" font-size="20">130</text>
            <text x="10" y="20" font-size="12" fill="#333">Predicted →</text>
            <text x="10" y="36" font-size="12" fill="#333">Actual ↓</text>
            <text x="50" y="22" font-size="12" fill="#333">Happy</text>
            <text x="140" y="22" font-size="12" fill="#333">Sad</text>
          </svg>
        </div>
      </div>
      <p style="margin-top:.6rem;color:var(--muted)">Tip: Export your matplotlib / seaborn plots from Colab as PNG and replace these SVG placeholders in the repo.</p>
    </section>

    <section id="results">
      <h2>Results</h2>
      <p>Summarized metrics from Relaxify baseline experiments (example numbers — replace with your notebook outputs):</p>
      <table>
        <thead><tr><th>Experiment</th><th>Val Accuracy</th><th>Notes</th></tr></thead>
        <tbody>
          <tr><td>AlexNet baseline</td><td>68%</td><td>No augmentation, lr=1e-3, batch=32</td></tr>
          <tr><td>AlexNet + augmentation</td><td>73%</td><td>Random flip, rotation</td></tr>
          <tr><td>AlexNet lr=1e-4</td><td>75%</td><td>Lower learning rate improved stability</td></tr>
          <tr><td>ResNet18 (transfer)</td><td>80%</td><td>Better generalization but heavier model</td></tr>
        </tbody>
      </table>

      <p style="margin-top:.5rem">All trained model weights, training logs, and notebooks should be included in the GitHub repository as well (see README).</p>
    </section>

    <section id="takeaways">
      <h2>Takeaways & Next Steps</h2>
      <ul>
        <li>AlexNet is a solid baseline for facial emotion detection — quick to train on moderate datasets.</li>
        <li>Data augmentation and lower learning rates consistently improved validation performance.</li>
        <li>Consider transfer learning (ResNet18) for production-quality accuracy.</li>
        <li>Next: build a lightweight on-device model, integrate with Relaxify UI, and run user studies for efficacy.</li>
      </ul>

      <p><strong>How to publish on GitHub Pages</strong> (quick steps):</p>
      <ol>
        <li>Create a new GitHub repo (e.g., <code>victorya/relaxify</code>).</li>
        <li>Add this <code>index.html</code> to the repository root.</li>
        <li>Commit & push. In repository settings, enable Pages from the <code>main</code> branch &amp; root folder.</li>
        <li>Your site will be available at <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code> within a minute or two.</li>
      </ol>

      <p style="margin-top:.6rem">Want me to create the GitHub repo structure (README.md, Colab link, sample plots) and a ZIP of the site ready to push? I can generate those files next.</p>
    </section>

    <footer>
      Relaxify • Victorya Linder • Data Science &amp; Analytics — Fall 2025
    </footer>

  </main>

  <script>
    // small enhancement: toggle visible face styles (no external images required)
    // This script could later be expanded to show live webcam thumbnail and predictions.
    (function(){
      const happy = document.getElementById('happy');
      const sad = document.getElementById('sad');
      // make both visible; in real UI you'd highlight one based on model prediction
      happy.style.opacity = 1;
      sad.style.opacity = 1;
    })();
  </script>
</body>
</html>
